# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License.

# Pull request against these branches will trigger this build
pr:
- master
- staging
- contrib
- correctness

# Any commit to this branch will trigger the build.
trigger:
- master
- staging
- contrib
- correctness

###################################################################################################
# The pre-requisite for these jobs is to have 4 GPUs on your virtual machine (K80 or better)
# Jobs are daisy-chained by stages - more relevant stages come first (the ones we're currently 
# working on):
#    - if they fail no point in running anything else
#    - each stage _can_ have parallel jobs but that's not always needed for fast execution
###################################################################################################

jobs:

###################################################################################################
# Stage 1: Setup
###################################################################################################

- job: setup
  timeoutInMinutes: 10
  displayName: Setup
  pool:
    name: deepseismicagentpool
  steps:
  - bash: |
      # terminate as soon as any internal script fails
      set -e
      
      echo "Running setup..."
      pwd
      ls
      git branch
      uname -ra

      ./scripts/env_reinstall.sh
  
      # use hardcoded root for now because not sure how env changes under ADO policy
      DATA_ROOT="/home/alfred/data_dynamic"

      ./tests/cicd/src/scripts/get_data_for_builds.sh ${DATA_ROOT}

      # copy your model files like so - using dummy file to illustrate
      azcopy --quiet --source:https://$(storagename).blob.core.windows.net/models/model --source-key $(storagekey) --destination /home/alfred/models/your_model_name


###################################################################################################
# Stage 2: fast unit tests
###################################################################################################

- job: scripts_unit_tests_job
  dependsOn: setup
  timeoutInMinutes: 5
  displayName: Unit Tests
  pool:
    name: deepseismicagentpool
  steps:
  - bash: |
      set -e
      echo "Starting scripts unit tests"
      source activate seismic-interpretation
      pytest --durations=0 tests/
      echo "Script unit test job passed"

- job: cv_lib_unit_tests_job
  dependsOn: scripts_unit_tests_job
  timeoutInMinutes: 5
  displayName: cv_lib Unit Tests
  pool:
    name: deepseismicagentpool
  steps:
  - bash: |
      set -e
      echo "Starting cv_lib unit tests"
      source activate seismic-interpretation
      pytest --durations=0 cv_lib/tests/
      echo "cv_lib unit test job passed"


###################################################################################################
# Stage 3: Dutch F3 patch models: deconvnet, unet, HRNet patch depth, HRNet section depth
# CAUTION: reverted these builds to single-GPU leaving new multi-GPU code in to be reverted later
###################################################################################################

- job: dutchf3_patch
  dependsOn: cv_lib_unit_tests_job
  timeoutInMinutes: 20
  displayName: Dutch F3 patch local
  pool:
    name: deepseismicagentpool
  steps:
  - bash: |

      source activate seismic-interpretation

      # disable auto error handling as we flag it manually
      set +e

      cd experiments/interpretation/dutchf3_patch/local

      # Create a temporary directory to store the statuses
      dir=$(mktemp -d)

      pids=
      # export CUDA_VISIBLE_DEVICES=0
      { python train.py 'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' 'TRAIN.END_EPOCH' 1 'TRAIN.SNAPSHOTS' 1 \
                      'TRAIN.DEPTH' 'none' \
                      'OUTPUT_DIR' 'output' 'TRAIN.MODEL_DIR' 'no_depth' \
                      --cfg=configs/patch_deconvnet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"
      # export CUDA_VISIBLE_DEVICES=1
      { python train.py 'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' 'TRAIN.END_EPOCH' 1 'TRAIN.SNAPSHOTS' 1 \
                      'TRAIN.DEPTH' 'section' \
                      'OUTPUT_DIR' 'output' 'TRAIN.MODEL_DIR' 'section_depth' \
                      --cfg=configs/unet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"
      # export CUDA_VISIBLE_DEVICES=2
      { python train.py 'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' 'TRAIN.END_EPOCH' 1 'TRAIN.SNAPSHOTS' 1 \
                      'TRAIN.DEPTH' 'section' \
                      'OUTPUT_DIR' 'output' 'TRAIN.MODEL_DIR' 'section_depth' \
                      --cfg=configs/seresnet_unet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"
      # export CUDA_VISIBLE_DEVICES=3
      { python train.py 'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' 'TRAIN.END_EPOCH' 1 'TRAIN.SNAPSHOTS' 1 \
                      'TRAIN.DEPTH' 'section' \
                      'MODEL.PRETRAINED' '/home/alfred/models/hrnetv2_w48_imagenet_pretrained.pth' \
                      'OUTPUT_DIR' 'output' 'TRAIN.MODEL_DIR' 'section_depth' \
                      --cfg=configs/hrnet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"

      wait $pids || exit 1

      # check if any of the models had an error during execution
      # Get return information for each pid
      for file in "$dir"/*; do
        printf 'PID %d returned %d\n' "${file##*/}" "$(<"$file")"
        [[ "$(<"$file")" -ne "0" ]] && exit 1 || echo "pass"
      done

      # Remove the temporary directory
      rm -r "$dir"

      echo "All models finished training - start scoring"

      # Create a temporary directory to store the statuses
      dir=$(mktemp -d)

      pids=
      # export CUDA_VISIBLE_DEVICES=0
      # find the latest model which we just trained
      model_dir=$(ls -td output/patch_deconvnet/no_depth/* | head -1)
      model=$(ls -t ${model_dir}/*.pth | head -1)
      # try running the test script
      { python test.py 'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' \
                     'TEST.MODEL_PATH' ${model} \
                     --cfg=configs/patch_deconvnet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"
      # export CUDA_VISIBLE_DEVICES=1
      # find the latest model which we just trained
      model_dir=$(ls -td output/unet/section_depth/* | head -1)
      model=$(ls -t ${model_dir}/*.pth | head -1)

      # try running the test script
      { python test.py  'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' \
                      'TEST.MODEL_PATH' ${model} \
                      --cfg=configs/unet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"
      # export CUDA_VISIBLE_DEVICES=2
      # find the latest model which we just trained
      model_dir=$(ls -td output/seresnet_unet/section_depth/* | head -1)
      model=$(ls -t ${model_dir}/*.pth | head -1)

      # try running the test script
      { python test.py 'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' \
                     'TEST.MODEL_PATH'  ${model} \
                     --cfg=configs/seresnet_unet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"
      # export CUDA_VISIBLE_DEVICES=3
      # find the latest model which we just trained
      model_dir=$(ls -td output/hrnet/section_depth/* | head -1)
      model=$(ls -t ${model_dir}/*.pth | head -1)

      # try running the test script
      { python test.py 'DATASET.ROOT' '/home/alfred/data_dynamic/dutch_f3/data' \
                     'MODEL.PRETRAINED' '/home/alfred/models/hrnetv2_w48_imagenet_pretrained.pth' \
                     'TEST.MODEL_PATH'  ${model} \
                     --cfg=configs/hrnet.yaml --debug ; echo "$?" > "$dir/$BASHPID"; }
      pids+=" $!"

      # wait for completion
      wait $pids || exit 1

      # check if any of the models had an error during execution
      # Get return information for each pid
      for file in "$dir"/*; do
        printf 'PID %d returned %d\n' "${file##*/}" "$(<"$file")"
        [[ "$(<"$file")" -ne "0" ]] && exit 1 || echo "pass"
      done

      # Remove the temporary directory
      rm -r "$dir"

      echo "PASSED"